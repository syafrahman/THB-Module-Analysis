{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9b43c73-214e-4ee0-95e2-e289d411589e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "df = pd.read_pickle(\"../../../data/processed/df/df_00.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf639179-31cd-4de2-ae77-55a775e7a86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stopwords = set([\"Seite\", \"xx\", \"h\", \"vorher\", \"davon\", \"oder\", \"z.\", \"B.\", \"u.\", \"a.\", \"of\", \"to\", \"and\", '\\uf0b7',\n",
    "    \"ggf.\", \"jeder\", \"'s\", \"or\", \"be\", \"via\", \"z.B.\", \"bzw.\", \"-Das\", \"the\",\"(=\"])\n",
    "for word in custom_stopwords:\n",
    "    nlp.Defaults.stop_words.add(word)\n",
    "    nlp.vocab[word].is_stop = True\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    text = ' '.join(text.split())\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    tokens = [token.text for token in doc if not\n",
    "              (token.is_stop or \n",
    "               token.like_email or \n",
    "               token.like_url or \n",
    "               token.is_punct or \n",
    "               token.like_num or\n",
    "               (len(token.text) == 1 and ord(token.text) < 128))]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Assuming df is your DataFrame with a column named 'ExtractedText'\n",
    "# Apply the preprocess_text function to each row in 'ExtractedText'\n",
    "# The result is stored in a new column 'CleanedText'\n",
    "df['CleanedText'] = df['ExtractedText'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a5074fa-863e-431b-9f82-2716d2a26b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list= ['AOG-0-PropAO', 'AOG-0-PropAO', 'AOG-1-AnPhysio', 'AOG-1-AnPhysio', 'AOG-1-AnPhysio', 'AOG-1-IngMa-1', 'AOG-1-IngMa-1', 'AOG-1-EP-IW-WI', 'AOG-1-EP-IW-WI', 'AOG-1-EP-IW-WI', 'AOG-1-PhysGL-1', 'AOG-1-PhysGL-1', 'AOG-1-PhysGL-1', 'AOG-1-SubjRefra-1', 'AOG-1-SubjRefra-1', 'AOG-1-TO-1', 'AOG-1-TO-1', 'AOG-2-IngMa-2', 'AOG-2-IngMa-2', 'AOG-2-KLAnp-1', 'AOG-2-KLAnp-1', 'AOG-2-PhysGL-2', 'AOG-2-PhysGL-2', 'AOG-2-PhysGL-2', 'AOG-2-SkiOph', 'AOG-2-SkiOph', 'AOG-2-SubjRefra-2', 'AOG-2-SubjRefra-2', 'AOG-2-TO-2', 'AOG-1-TO-1', 'AOG-2-TO-2', 'AOG-3-ETG-1', 'AOG-3-ETG-1', 'AOG-3-KLAnp-2', 'AOG-3-KLAnp-2', 'AOG-3-KuF', 'AOG-3-KuF', 'AOG-3-KuF', 'AOG-3-OG-1', 'AOG-3-OG-1', 'AOG-3-Path', 'AOG-3-Path', 'AOG-3-Path', 'AOG-3-SubjRefra-3', 'AOG-3-SubjRefra-3', 'AOG-4-ETG-2', 'AOG-4-ETG-2', 'AOG-4-KLAnp-3', 'AOG-4-KLAnp-3', 'AOG-4-MT', 'AOG-4-MT', 'AOG-4-MT', 'ISBN-13', '662-', 'AOG-4-OG-2', 'AOG-4-OG-2', 'AOG-4-OG-2', 'AOG-4-Opt', 'AOG-4-Opt', 'AOG-4-SubjRefra-4', 'AOG-4-SubjRefra-4', 'AOG-5-OTS', 'AOG-5-OTS', 'AOG-5-WFFO-1', 'AOG-5-WFFO-1', 'AOG-5-WP1-Werk', 'AOG-5-WP1-Werk', 'AOG-5-WP1-Progr', 'AOG-5-WP1-Progr', 'AOG-5-PP', 'AOG-5-PP', 'AOG-6-Alt', 'AOG-6-Alt', 'AOG-6-KLAnp-4', 'AOG-6-KLAnp-4', 'AOG-6-WFFO-2', 'AOG-6-WFFO-2', 'AOG-6-WP2-BWLHWK', 'AOG-6-WP2-BWLHWK', 'AOG-6-WP3', '1-', 'AOG-6-WP3', '1-', 'AOG-6-WP3', '2-EntwS', 'AOG-6-WP3', '2-EntwS', '3-', '13-', '3-', 'AOG-6-WP3', '2-EntwS', 'AOG-6-WP4-KlinP', 'AOG-6-WP4-KlinP', 'AOG-6-WP2-DST', 'AOG-6-WP2-DST', 'AOG-6-WP3', '1-SRT', 'AOG-6-WP3', '1-SRT', 'AOG-6-WP3', '2-Spek', 'AOG-6-WP3', '2-Spek', 'AOG-6-WP4', '1-DBV', 'AOG-6-WP4', '1-DBV', 'AOG-6-WP4', '2-DvBG', 'AOG-6-WP4', '2-DvBG', 'AOG-7-F&E', 'AOG-7-F&E', 'AOG-7-LaOph', 'AOG-7-LaOph', 'AOG-7-LaOph', 'AOG-7-WP5', '1-', 'AOG-7-WP5', '1-', 'AOG-7-WP5', '1-', 'AOG-7-WP5', '2-VPD', 'AOG-7-WP5', '2-VPD', 'AOG-7-WP5', '1-MoL', 'AOG-7-WP5', '1-MoL', 'AOG-7-WP5', '2-BWL', 'AOG-7-WP5', '2-BWL', '352-', '2-', '3-', '978-', '42169-', '978-', 'ISBN-10', 'ISBN-10', 'ISBN-10', 'ISBN-10', '26194-X', '7923-', 'ISBN-10:0387773258', 'ISBN-10', '3-', 'ISBN-10', 'ISBN-10', 'WK1-L', 'WK2-L', '1-Vorlesung', '1:2011VU-gekerbten', 'FT1-L', '6-Achs-', '468-gliedrige', '4-gliedriger', '3D-Druck', '662e-book', 'ISBN-13', '26.-29.', '7-', '293-', '2-', '3-', 'ISBN-10', 'ISBN-10', 'ISBN-10', 'ISBN-10', 'ISO-9241', '03932-', '26194-X', '7923-', 'ISBN-10:0387773258', 'ISBN-10', '978-', '978-', '528-', '486-', 'ISBN-10', 'ISBN-10', 'ISBN-10', 'ISBN-10', 'ISBN-13', 'ISBN-13', 'ISBN-10', 'ISBN-10', '3-', 'ISBN-10', 'ISBN-10', '978-', 'ISBN-13', 'ISBN-13', '802.11-Netzen', '446-', '642-', '3-', 'Z2hoch-n', 'ISBN-13', 'B1-B2', 'ISBN-13', '27000-Reihe', '63497-', '26194-X', 'ISBN-13', '662e-book', 'ISBN-13', 'AI-DB1', 'AI-DB2', 'AI-DB3', 'AI-DB4', 'AI-DB5', 'AI-DB6', '7-Anwenderprogramms', 'S7-Projektierung', '7-Anwenderbausteinen', 'S7-GRAPH', 'TP700-Projektierung', 'RS485-Übertragungstechnik', 'SPS-PV1', 'SPS-PV2', 'Bd-3', '6-', 'CO2-Gehalt', 'EIA-232', 'EIA-485', 'KNX-1', 'KNX-2', 'KNX-3', 'KNX-4', 'KNX-5', '8051-Mikrocontrollers', '662e-book', 'ISBN-13', '7-Laborarbeitsplatz', '7-Operator', '7-Konfigurierung', '7-Konfigurierung', '7-Inbetriebnahme', '978-', '978-', '3-', 'Loop-2', '80-/20-Prinzipien', '3-', 'ISBN-10', 'ISBN-13', 'ISBN-10', 'ISBN-13', 'ISBN-10', 'ISBN-13', 'ISBN-13', '978-', 'ISBN-13', '63497-X.', '978-', 'Cortex-M3', 'ISBN-13', 'ISBN-10978', '1-', 'ISBN-10', 'ISBN-13', 'ISBN-10', 'ISBN-13', 'IT-Grundschutz1', 'IT-Grundschutz-Kompendium2', 'BSI-Standard-200', '4-Business-Continuity-Management', 'bsi-standard-200-', '978-', '0-', 'ISBN-13', '978-', 'ISBN-13', '662-', '3-', '978-', '3-', '658-', '21151-', 'Xv3-Zertifikate', 'ISBN-10', 'ISBN-13', 'Thttps://bmakewiki.th-brandenburg.de1', '80-', '/20-Prinzipien', 'customer-vendor-integration-s4-hana', 'inno-vations-with-sap-s4hana', 'networks-book.pdf1', 'contents-1.html1', '1986-', 'ISBN-13', 'ISBN-13', 'ISBN-13', 'ISBN-13', '978-', 'ISBN-13', '978-', 'ISBN-13', '978-', '55860-', 'ISBN-13', 'ISBN-10978', '1-', 'ISBN-10', '978-', '0-', 'ISBN-13', '978-', 'ISBN-13', '63497-X.', '978-', '3-', '94013-X', '0304-', '800-', '0306-Ü', '3D-Druck', 'M-5', 'WPM-7b', '87155-', '2-', '3-', 'ISBN-10', 'ISBN-10', 'ISBN-10', 'ISBN-10', 'ISO-9241', '03932-', '26194-X', '7923-', 'ISBN-10:0387773258', 'ISBN-10', '978-', '978-', '528-', '486-', 'ISBN-10', 'ISBN-10', 'ISBN-10', 'ISBN-10', 'ISBN-13', 'ISBN-13', 'ISBN-10', 'ISBN-10', '3-', 'ISBN-10', 'ISBN-10', '1-', '978-', 'ISBN-13', 'ISBN-13', '802.11-Netzen', '446-', '642-', 'isbn-13:978', 'ISBN-10', 'ISBN-10', 'ISBN-10', 'ISBN-10', 'ISBN-10', 'ISBN-10', 'ISBN-10', 'ISBN-10', '22150-', 'ISBN-13', 'ISBN-13', 'ISBN-13', 'ISBN-10', 'ISBN-13', 'ISBN-10', 'ISBN-13', 'ISBN-10', 'ISBN-13', 'ISBN-13', 'ISBN-13', 'ISBN-13', 'ISBN-13', 'ISBN-13', 'ISBN-10', 'ISBN-13:9783621275910', 'doi.org/10.1007/978-3-319-44162-7.', 'T1-', 'T2-Bestimmung', 'V1-V6', 'V1-V6', 'Z2-hoch-n', '3-', '3-', '133-', '9241-xxx', '446-', '0-', '201-', '63497-X.', '978-', 'B1-B2', '27000-Reihe', 'B1-B2', 'B2-C1', 'IZO-803', 'IZO-804']\n",
    "\n",
    "token_list_lower = [word.lower() for word in token_list]\n",
    "removed_hyphen_digits_texts = []\n",
    "hyphen_digits_texts = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    text = row['CleanedText']\n",
    "    doc = nlp(text)  # Process with the German model\n",
    "\n",
    "    removed_hyphen_digits_tokens = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.text in token_list_lower:\n",
    "            hyphen_digits_texts.append(token.text)\n",
    "            continue\n",
    "        else:\n",
    "            removed_hyphen_digits_tokens.append(token.text)\n",
    "\n",
    "    removed_hyphen_digits_text = ' '.join(removed_hyphen_digits_tokens)\n",
    "    removed_hyphen_digits_texts.append(removed_hyphen_digits_text)\n",
    "\n",
    "df['ProcessedText'] = removed_hyphen_digits_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "041cd119-9dc8-4e71-bd74-c30798b3f69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_texts = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    text = row['ProcessedText']\n",
    "    doc = nlp(text)  # Process with the German model\n",
    "\n",
    "    updated_tokens = []\n",
    "\n",
    "    # Iterate through the tokens\n",
    "    for token in doc:\n",
    "        if token.text.startswith(\"-\"):\n",
    "            updated_tokens.append(token.text[1:])\n",
    "        elif token.text.endswith(\"-\"):\n",
    "            updated_tokens.append(token.text[:-1])\n",
    "        else:\n",
    "            updated_tokens.append(token.text)  # Append the token as is if it doesn't start with hyphen\n",
    "\n",
    "    updated_text = ' '.join(updated_tokens)\n",
    "    updated_texts.append(updated_text)\n",
    "    \n",
    "df['UpdatedText'] = updated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e6a366e-7592-4823-9463-84cb61137326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text_german(text):\n",
    "    doc = nlp(text)\n",
    "    lemmatized_tokens = [token.lemma_ for token in doc]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Apply the lemmatization function to the 'cleanedtext' column\n",
    "df['UpdatedText'] = df['UpdatedText'].apply(lemmatize_text_german)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d15f2d5b-3928-42a6-b1d1-873f4c0d886d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df['UpdatedText']\n",
    "\n",
    "def gen_words(texts):\n",
    "    final = []\n",
    "    for text in texts:\n",
    "        new = text.split()  # Split the text into words\n",
    "        final.append(new)\n",
    "    return final\n",
    "\n",
    "data_words = gen_words(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "331d7133-3528-4503-b2b0-552a2b13385c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Words'] = data_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad96d1d7-b16f-4c89-bf0b-461302892e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"df_04.pkl\"\n",
    "df.to_pickle(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb9ccce-8636-4640-8266-d43157d51f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"df_04.pkl\")\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
